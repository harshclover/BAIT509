Answer the following questions, and upload your results to your github repo. Remember, your answers do not have to be correct to earn participation points!

Bagging is a special case of random forests under which case?
When the subset of variables chosen for prediction is equal to the set of all variables i.e. m = p.
What are the hyperparameters we can control for random forests?
We can control the tree size i.e. n, and we can control the subset of variables we want to generate our trees i.e. m which is generally chosen to be square root of p.
Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
(1,0), (1,2), (1,5) - NO, since (1,2) acts as a single entity and not a combination of two entities 1 and 2. So we cannot split it to make prediction.
(1,2), (2,0) - NO, m is not equal to p in this case, the number of predictions is not equal to the sample size.
(1,2), (1,2), (1,5) - Yes, m is equal to p in this case, also the number of predictions is equal to the sample size. This case overcomes the anamolies in part 1 and 2. Thus, it is a valid bootstapped data set.
For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?
Since (2,0) is not present in our bootstrapped data, therefore it is OOB.
You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
Regression: your trees make the following four predictions: 1,1,3,3. Since, these are a bunch of quantitative variables, thus my prediction would be the mean of the dataset. Therefore, 2 will be the prediction.
Classification: your trees make the following four predictions: "A", "A", "B", "C". Since this is a categorical variable, our prediction will be the mode of dataset, i.e. A in this case.
